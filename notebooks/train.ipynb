{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\gvillanueva\\Desktop\\Technician_2022_2023\\Projects\\Mental_health_COVID19\\ds2024\\lib\\site-packages\\keras\\src\\losses.py:2940: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import linear_model\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), os.pardir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "from src.config import CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check/Create path\n",
    "for target in  CONFIG.outcomes:\n",
    "    if target != \"G_HADSscore\":\n",
    "        for n_features in CONFIG.n_features:\n",
    "            path_cv_data = CONFIG.path_results+\"/cv_data/\"+target+\"/\" +  str(n_features) + \"/\"\n",
    "            if os.path.exists(path_cv_data):\n",
    "                shutil.rmtree(path_cv_data)\n",
    "                os.makedirs(path_cv_data)\n",
    "            else:\n",
    "                os.makedirs(path_cv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadind data ...\n",
      "Target discretization ...\n",
      "Number of features = 161\n",
      "--------------------------------------------------------\n",
      "Number of samples = 9291\n",
      "--------------------------------------------------------\n",
      "Number of categorical features = 114\n",
      "--------------------------------------------------------\n",
      "Number of numerical features = 47\n",
      "--------------------------------------------------------\n",
      "Number of features with missing values = 31\n",
      "Percentage of missing data =  0.34\n",
      "--------------------------------------------------------\n",
      "BATCH: 1 | CV: 1\n",
      "--------------------------------------------------------\n",
      "Imputing missing values ...\n",
      "One-hot-encoding ...\n",
      "Scaling data ...\n",
      "--------------------------------------------------------\n",
      "target = G_depressionscore\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "target = G_anxietyscore\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "target = G_totalscore\n",
      "--------------------------------------------------------\n",
      "BATCH: 1 | CV: 2\n",
      "--------------------------------------------------------\n",
      "Imputing missing values ...\n",
      "One-hot-encoding ...\n",
      "Scaling data ...\n",
      "--------------------------------------------------------\n",
      "target = G_depressionscore\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "target = G_anxietyscore\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "target = G_totalscore\n",
      "--------------------------------------------------------\n",
      "BATCH: 1 | CV: 3\n",
      "--------------------------------------------------------\n",
      "Imputing missing values ...\n",
      "One-hot-encoding ...\n",
      "Scaling data ...\n",
      "--------------------------------------------------------\n",
      "target = G_depressionscore\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "target = G_anxietyscore\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "target = G_totalscore\n",
      "--------------------------------------------------------\n",
      "BATCH: 1 | CV: 4\n",
      "--------------------------------------------------------\n",
      "Imputing missing values ...\n",
      "One-hot-encoding ...\n",
      "Scaling data ...\n",
      "--------------------------------------------------------\n",
      "target = G_depressionscore\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "target = G_anxietyscore\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "target = G_totalscore\n",
      "--------------------------------------------------------\n",
      "BATCH: 1 | CV: 5\n",
      "--------------------------------------------------------\n",
      "Imputing missing values ...\n",
      "One-hot-encoding ...\n",
      "Scaling data ...\n",
      "--------------------------------------------------------\n",
      "target = G_depressionscore\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "target = G_anxietyscore\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "target = G_totalscore\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ########### Load data ############\n",
    "print('Loadind data ...')\n",
    "df_data, df_codebook  = utils.read_data(CONFIG.path_dataset, CONFIG.path_codebook)\n",
    "df_data = utils.data_preprocessing(df_data,df_codebook,CONFIG.non_relevant_vars, CONFIG.outcomes)\n",
    "\n",
    "############ Set target ############\n",
    "print('Target discretization ...')\n",
    "X,y_4 = utils.set_target(df_data,CONFIG.outcomes,CONFIG.thr_disc_1,CONFIG.thr_disc_2)\n",
    "y_3 = y_4.drop('G_HADSscore',axis=1)\n",
    "\n",
    "print('Number of features =',len(X.columns))\n",
    "print('--------------------------------------------------------')\n",
    "\n",
    "print('Number of samples =',X.shape[0])\n",
    "print('--------------------------------------------------------')\n",
    "\n",
    "############ Data encoding ############\n",
    "X, num_vars, categ_vars = utils.data_encoding(X,CONFIG.additional_categ_var)\n",
    "\n",
    "print('Number of categorical features =',len(categ_vars))\n",
    "print('--------------------------------------------------------')\n",
    "print('Number of numerical features =',len(num_vars))\n",
    "print('--------------------------------------------------------')\n",
    "\n",
    "missing_data=X.isna().sum()\n",
    "missing_data = missing_data[missing_data != 0]\n",
    "\n",
    "print('Number of features with missing values =', len(missing_data))\n",
    "print('Percentage of missing data = ', round(100*missing_data.sum()/(X.shape[0]*X.shape[1]),2))\n",
    "print('--------------------------------------------------------')\n",
    "\n",
    "cv_collection = []\n",
    "index_scores = []\n",
    "\n",
    "# Set outer cross-validation\n",
    "for i in range(CONFIG.outer_cv):\n",
    "    cv_t = StratifiedKFold(n_splits=CONFIG.inner_cv, shuffle=True)\n",
    "    cv_collection.append(cv_t)\n",
    "\n",
    "for count in range(len(cv_collection)):\n",
    "    for cv_i, (train_index, test_index) in enumerate(cv_collection[count].split(X, y_3[y_3.columns[0]])):\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train_3, y_test_3 = y_3.iloc[train_index,:], y_3.iloc[test_index,:]\n",
    "\n",
    "        index_scores.append('BATCH: {} | CV: {} '.format(count + 1,cv_i+1))\n",
    "\n",
    "        print('BATCH: {} | CV: {}'.format(count + 1,cv_i+1))\n",
    "        print('--------------------------------------------------------')\n",
    "\n",
    "        ############ Imputation of missing values ############\n",
    "        print('Imputing missing values ...')\n",
    "        X_train, X_test = utils.data_imputation(X_train, X_test,categ_vars)\n",
    "\n",
    "        ############ One-hot-encoding ############\n",
    "        print('One-hot-encoding ...')\n",
    "        X_train, X_test = utils.data_one_hot_encoding(X_train, X_test, CONFIG.categ_nominal_var)\n",
    "\n",
    "        ############ Scale data ############\n",
    "        print('Scaling data ...')\n",
    "        X_train_scaled, X_test_scaled = utils.data_scale(X_train, X_test)\n",
    "\n",
    "        for target in y_3.columns:\n",
    "            \n",
    "            print('--------------------------------------------------------')\n",
    "            print('target =', target)\n",
    "            print('--------------------------------------------------------')\n",
    "\n",
    "            y_train = pd.DataFrame(y_train_3[target])\n",
    "            y_test = pd.DataFrame(y_test_3[target])\n",
    "\n",
    "            # Save full data\n",
    "            data_train_full = pd.concat([y_train,X_train], axis = 1)\n",
    "            data_test_full = pd.concat([y_test,X_test], axis = 1)\n",
    "\n",
    "            data_train_full_scaled = pd.concat([y_train,X_train_scaled], axis = 1)\n",
    "            data_test_full_scaled = pd.concat([y_test,X_test_scaled], axis = 1)\n",
    "\n",
    "            data = pd.concat([data_train_full,data_test_full],keys=['train','test'])\n",
    "            data.to_csv(CONFIG.path_results+\"/cv_data/\"+target+ \"/data_batch_\"+str(count+1)+\"_cv_\"+str(cv_i+1)+\".csv\")\n",
    "\n",
    "            data_scaled = pd.concat([data_train_full_scaled,data_test_full_scaled],keys=['train','test'])\n",
    "            data_scaled.to_csv(CONFIG.path_results+\"/cv_data/\"+target + \"/data_scaled_batch_\"+str(count+1)+\"_cv_\"+str(cv_i+1)+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check/Create path\n",
    "for target in  CONFIG.outcomes:\n",
    "    if target != \"G_HADSscore\":\n",
    "        for n_features in CONFIG.n_features:\n",
    "            path_cv_data = CONFIG.path_results+\"/cv_data/\"+target+\"/\" +  str(n_features) + \"/\"\n",
    "            if os.path.exists(path_cv_data):\n",
    "                pass\n",
    "            else:\n",
    "                os.makedirs(path_cv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check/Create path\n",
    "for out in  CONFIG.outcomes:\n",
    "    if out != 'G_HADSscore':\n",
    "        for n_features in  CONFIG.n_features:\n",
    "            if os.path.exists(CONFIG.path_results+\"models/\"+out+\"/\"+model_name+\"/\"+str(n_features)+\"/\"):\n",
    "                pass\n",
    "            else:\n",
    "                os.makedirs(CONFIG.path_results+\"models/\"+out+\"/\"+model_name+\"/\"+str(n_features)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBoost\n"
     ]
    }
   ],
   "source": [
    "# ML model\n",
    "model_name = CONFIG.clf_name \n",
    "print(\"Model: \" + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Check/Create path\n",
    "for out in  CONFIG.outcomes:\n",
    "    if out != 'G_HADSscore':\n",
    "        \n",
    "        if os.path.exists(CONFIG.path_results+\"results/\"+out+\"/\"+model_name+\"/\"):\n",
    "            shutil.rmtree(CONFIG.path_results+\"results/\"+out+\"/\"+model_name+\"/\")\n",
    "            os.makedirs(CONFIG.path_results+\"results/\"+out+\"/\"+model_name+\"/\")\n",
    "        else:\n",
    "            os.makedirs(CONFIG.path_results+\"results/\"+out+\"/\"+model_name+\"/\")\n",
    "\n",
    "        for n_features in  CONFIG.n_features:\n",
    "            if os.path.exists(CONFIG.path_results+\"models/\"+out+\"/\"+model_name+\"/\"+str(n_features)+\"/\"):\n",
    "                shutil.rmtree(CONFIG.path_results+\"models/\"+out+\"/\"+model_name+\"/\"+str(n_features)+\"/\")\n",
    "                os.makedirs(CONFIG.path_results+\"models/\"+out+\"/\"+model_name+\"/\"+str(n_features)+\"/\")\n",
    "            else:\n",
    "                os.makedirs(CONFIG.path_results+\"models/\"+out+\"/\"+model_name+\"/\"+str(n_features)+\"/\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model keras implementation\n",
    "def create_model(n_layers, learning_rate, l1, l2, act, dropout, n_features):                      \n",
    "    '''This is a model generating function so that we can search over neural net \n",
    "    parameters and architecture'''\n",
    "    \n",
    "    opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
    "                                                    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # for the firt layer we need to specify the input dimensions\n",
    "    first=True\n",
    "\n",
    "    n_neurons = np.random.choice([25,50,100], size=n_layers)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        if first:\n",
    "            model.add(Dense(n_neurons[i], input_dim=n_features, activation=act, kernel_regularizer=reg))\n",
    "            first=False\n",
    "        else: \n",
    "            model.add(Dense(n_neurons[i], activation=act, kernel_regularizer=reg))\n",
    "        if dropout!=0:\n",
    "            model.add(Dropout(dropout))     \n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************\n",
      "********************************************************\n",
      "                  target = G_depressionscore\n",
      "********************************************************\n",
      "********************************************************\n",
      "[batch, cv, n_features] = [1, 1, 100]\n",
      "Selecting features...\n",
      "Optimizing model... \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/gvillanueva/Desktop/Technician_2022_2023/Projects/Mental_health_COVID19/models/G_depressionscore/XGBoost/100/model_batch_1_cv_1.sav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m     best_model\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;241m.\u001b[39msave(CONFIG\u001b[38;5;241m.\u001b[39mpath_results\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtarget\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mmodel_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(n_features)\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/model_batch_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(batch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cv_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(cv\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m     \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_results\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/model_batch_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_cv_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.sav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     92\u001b[0m au \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(best_score[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank_test_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\gvillanueva\\Desktop\\Technician_2022_2023\\Projects\\Mental_health_COVID19\\ds2024\\lib\\site-packages\\joblib\\numpy_pickle.py:552\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    550\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[1;32m--> 552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    553\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/gvillanueva/Desktop/Technician_2022_2023/Projects/Mental_health_COVID19/models/G_depressionscore/XGBoost/100/model_batch_1_cv_1.sav'"
     ]
    }
   ],
   "source": [
    "for batch in range(CONFIG.outer_cv):\n",
    "    for cv in range(CONFIG.inner_cv):\n",
    "        \n",
    "        for target in CONFIG.outcomes:\n",
    "            \n",
    "            if target != 'G_HADSscore':\n",
    "                \n",
    "                print('********************************************************')\n",
    "                print('********************************************************')\n",
    "                print('                  target =', target)\n",
    "                print('********************************************************')\n",
    "                print('********************************************************')\n",
    "\n",
    "                path_data_scaled = CONFIG.path_results+\"/cv_data/\"+target + \"/data_scaled_batch_\"+str(batch+1)+\"_cv_\"+str(cv+1)+\".csv\"\n",
    "                data_scaled = pd.read_csv(path_data_scaled, index_col=[0,1])\n",
    "\n",
    "                data_train_scaled = data_scaled.loc['train']\n",
    "                y_train = data_train_scaled[target]\n",
    "                X_train_scaled = data_train_scaled.drop([target],axis=1)\n",
    "\n",
    "                data_test_scaled = data_scaled.loc['test']\n",
    "                y_test = data_test_scaled[target]\n",
    "                X_test_scaled = data_test_scaled.drop([target],axis=1)\n",
    "\n",
    "                path_data = CONFIG.path_results+\"/cv_data/\"+target + \"/data_batch_\"+str(batch+1)+\"_cv_\"+str(cv+1)+\".csv\"\n",
    "                data = pd.read_csv(path_data, index_col=[0,1])\n",
    "\n",
    "                data_train = data.loc['train']\n",
    "                y_train = data_train[target]\n",
    "                X_train = data_train.drop([target],axis=1)\n",
    "\n",
    "                data_test = data.loc['test']\n",
    "                y_test = data_test[target]\n",
    "                X_test = data_test.drop([target],axis=1)\n",
    "\n",
    "                # Set inner CV\n",
    "                cv_collection2 = []\n",
    "                cv_t2 = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "                cv_collection2.append(cv_t2)\n",
    "                cv_iter = []\n",
    "\n",
    "                for count2 in range(len(cv_collection2)):\n",
    "                    for cv_i2, (train_index, test_index) in enumerate(cv_collection2[count2].split(X_train_scaled, y_train)):\n",
    "                        cv_iter.append((train_index, test_index))\n",
    "                \n",
    "                test_scores = []\n",
    "                    \n",
    "                for n_features in CONFIG.n_features:\n",
    "                    print('[batch, cv, n_features] = [' + str(batch+1) + \", \" + str(cv+1) + \", \" + str(n_features) + \"]\")\n",
    "                    print('Selecting features...')\n",
    "                    X_train_red, X_train_scaled_red, X_test_red, X_test_scaled_red = utils.data_feature_selection(X_train, X_train_scaled, X_test, X_test_scaled, y_train, y_test, CONFIG.algorithm, n_features)\n",
    "\n",
    "                    # Save full data\n",
    "                    data_train_full = pd.concat([y_train,X_train_red], axis = 1)\n",
    "                    data_test_full = pd.concat([y_test,X_test_red], axis = 1)\n",
    "\n",
    "                    data_train_full_scaled = pd.concat([y_train,X_train_scaled_red], axis = 1)\n",
    "                    data_test_full_scaled = pd.concat([y_test,X_test_scaled_red], axis = 1)\n",
    "\n",
    "                    data = pd.concat([data_train_full,data_test_full],keys=['train','test'])\n",
    "                    data.to_csv(CONFIG.path_results+\"/cv_data/\"+target+\"/\" + str(n_features) + \"/data_batch_\"+str(batch+1)+\"_cv_\"+str(cv+1)+\".csv\")\n",
    "\n",
    "                    data_scaled = pd.concat([data_train_full_scaled,data_test_full_scaled],keys=['train','test'])\n",
    "                    data_scaled.to_csv(CONFIG.path_results+\"/cv_data/\"+target+\"/\" + str(n_features) + \"/data_scaled_batch_\"+str(batch+1)+\"_cv_\"+str(cv+1)+\".csv\")\n",
    "\n",
    "                    ############ Optimize model ############\n",
    "                    print('Optimizing model... ')\n",
    "\n",
    "                    # Choose model\n",
    "                    if CONFIG.clf_name == \"XGBoost\":\n",
    "                        clf = XGBClassifier(objective='multi:softprob',num_class=3,n_estimators = 50, learning_rate = 0.25, booster = 'gbtree', max_depth = 3)\n",
    "                    elif CONFIG.clf_name == \"Multi-Layer Perceptron\":        \n",
    "                        clf  = KerasClassifier(build_fn=create_model, n_features = 25, epochs=25, batch_size=20, verbose=0, n_layers=1 , learning_rate=0.01, l1=0.01, l2=0.01, act = 'relu', dropout=0)\n",
    "                    elif CONFIG.clf_name == \"Random Forest\":\n",
    "                        clf = RandomForestClassifier(n_estimators = 100)\n",
    "                    elif CONFIG.clf_name == \"Support Vector Machines\":\n",
    "                        clf = SVC(kernel=\"rbf\", probability=True, max_iter = 5000)\n",
    "                    elif CONFIG.clf_name == \"Naive Bayes\":\n",
    "                        clf = GaussianNB()\n",
    "                    elif CONFIG.clf_name == \"Logistic Regression\":\n",
    "                        clf = linear_model.LogisticRegression(multi_class='ovr', solver='liblinear')\n",
    "                    \n",
    "                    best_model, best_params, best_score = utils.grid_search(X_train_red, X_train_scaled_red, y_train, clf, CONFIG.clf_name, target,CONFIG.n_iter,cv_iter,CONFIG.n_features)\n",
    "\n",
    "                    # Save model\n",
    "                    if CONFIG.clf_name == \"Multi-Layer Perceptron\":\n",
    "                        best_model.model_.save(CONFIG.path_results+\"models/\"+target+\"/\"+model_name+\"/\"+str(n_features)+ \"/model_batch_\"+str(batch+1)+\"_cv_\"+str(cv+1)+\".keras\")\n",
    "                    else:\n",
    "                        joblib.dump(best_model, CONFIG.path_results+\"models/\"+target+\"/\"+model_name+\"/\"+str(n_features)+ \"/model_batch_\"+str(batch+1)+\"_cv_\"+str(cv+1)+\".sav\")\n",
    "      \n",
    "                    scores = []\n",
    "                    au = np.where(best_score['rank_test_score']==1)[0]\n",
    "                    for i in range(CONFIG.inner_cv):\n",
    "                        split_name = 'split'+ str(i) + '_test_score'\n",
    "                        au2 = best_score[split_name]\n",
    "                        scores.append(np.abs(au2[au])[0])\n",
    "\n",
    "                    test_scores.append(scores)\n",
    "                    cv_se = np.std(scores)/np.sqrt(5) \n",
    "\n",
    "                    print(\"mean score train: \", np.mean(scores),\" --- one-standard-error: \",cv_se)\n",
    "\n",
    "                    if CONFIG.clf_name == \"Multi-Layer Perceptron\":\n",
    "                        y_prob = best_model.predict(X_test_scaled_red)\n",
    "                        y_pred = np.argmax(y_prob,axis=1)\n",
    "                    else:\n",
    "                        y_pred = best_model.predict(X_test_scaled_red)\n",
    "                        y_prob = best_model.predict_proba(X_test_scaled_red)\n",
    "\n",
    "                    # Print metrics\n",
    "                    value2 = roc_auc_score(y_test, y_prob,multi_class=\"ovo\",average = \"macro\")\n",
    "                    #value2 = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "                    \n",
    "                    print(\"Score test: \", value2)\n",
    "                    print('--------------------------------------------------------')\n",
    "            \n",
    "                # save inner-cv scores\n",
    "                column_names = ['1','2','3','4','5']\n",
    "                performance_report = pd.DataFrame(test_scores, columns= column_names, index=CONFIG.n_features)\n",
    "                performance_report.to_csv(CONFIG.path_results +\"results/\"+target + \"/\" + model_name + \"/scores\"+str(batch+1)+\"_cv_\"+str(cv+1)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
